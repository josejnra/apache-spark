version: '3.7'

networks: 
  spark:

services: 

  spark:
    image: spark-yarn:3.1.2
    build:
      context: .
      dockerfile: Dockerfile
    container_name: spark
    networks:
      - spark
    hostname: localhost
    ports:
      - 8080:18080 # Spark History Server
      - 4040:4040 # Spark UI
      - 8088:8088 # Resource Manager UI
      - 8042:8042 # Node
      - 9864:9864 # Datanode port
      - 9870:9870 # HDFS UI
      - 8998:8998 # Livy UI
      - 8888:8888 # Jupyter Notebook
    volumes:
      - ./apps:/opt/apps # put your apps here and run spark-submit.sh (edit it)
      - ./hadoop-conf/core-site.xml:/opt/hadoop-3.3.1/etc/hadoop/core-site.xml
      - ./hadoop-conf/hdfs-site.xml:/opt/hadoop-3.3.1/etc/hadoop/hdfs-site.xml
      - ./hadoop-conf/mapred-site.xml:/opt/hadoop-3.3.1/etc/hadoop/mapred-site.xml
      - ./hadoop-conf/yarn-site.xml:/opt/hadoop-3.3.1/etc/hadoop/yarn-site.xml
      - ./spark-conf/spark-defaults.conf:/opt/spark-3.1.2-bin-without-hadoop/conf/spark-defaults.conf
      - ./livy-conf/livy.conf:/opt/apache-livy/conf/livy.conf
    command:
      - "-c"
      - |
        echo "Starting ssh service..."
        service ssh start

        echo "Formating the filesystem..."
        hdfs namenode -format

        echo "Starting NameNode, DataNode, Resource Manager and Node Manager..."
        hdfs --daemon start namenode
        hdfs --daemon start datanode
        yarn --daemon start resourcemanager
        yarn --daemon start nodemanager
        yarn --daemon start proxyserver

        echo "Creating the HDFS directories required to execute MapReduce jobs..."
        hdfs dfs -mkdir /user
        hdfs dfs -mkdir /user/$$(whoami)
        hdfs dfs -mkdir /spark-logs
        # hdfs dfs -chmod -R 777 /spark-logs

        echo "Starting spark..."
        export SPARK_DIST_CLASSPATH=$$(hadoop classpath)
        /opt/spark-$${SPARK_VERSION}-bin-without-hadoop/sbin/start-all.sh
        /opt/spark-$${SPARK_VERSION}-bin-without-hadoop/sbin/start-history-server.sh

        # echo "Starting livy..."
        # /opt/apache-livy/bin/livy-server start

        echo "Starting jupyter lab..."
        jupyter lab --allow-root --no-browser --NotebookApp.token=''

        tail -f /opt/hadoop-$${HADOOP_VERSION}/logs/*log

  # elasticsearch:
  #   image: elasticsearch:7.14.1
  #   container_name: elasticsearch
  #   networks:
  #     - spark
  #   environment:
  #     - discovery.type=single-node
  #   volumes:
  #     - ./elastic-data:/user/share/elasticsearch/data
  #   user: "1000:0"
  #   ports: 
  #     - 9200:9200

  # kibana:
  #   image: kibana:7.14.1
  #   container_name: kibana
  #   networks:
  #     - spark
  #   user: "1000:0"
  #   ports: 
  #     - 5601:5601

  # mysql:
  #   image: mysql:8.0.26
  #   container_name: mysql
  #   networks:
  #     - spark
  #   environment:
  #     MYSQL_ROOT_PASSWORD: "root"
  #     MYSQL_DATABASE: "mydb"
  #     MYSQL_USER: "user"
  #     MYSQL_PASSWORD: "password"
  #   volumes:
  #     - ./mysql-data:/var/lib/mysql
  #   command: --explicit_defaults_for_timestamp
