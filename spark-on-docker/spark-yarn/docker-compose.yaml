version: '3.7'

networks: 
  spark:

services: 

  spark:
    image: spark-yarn:3.1.2
    container_name: spark
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 8080:18080 # Spark History Server
      - 8088:8088 # Resource Manager UI
      - 8042:8042
      - 9864:9864 # Datanode port
      - 9870:9870 # HDFS UI
    volumes: 
      - ./apps:/opt/apps
      - ./conf/core-site.xml:/home/hadoop/hadoop-3.3.1/etc/hadoop/core-site.xml
      - ./conf/hdfs-site.xml:/home/hadoop/hadoop-3.3.1/etc/hadoop/hdfs-site.xml
      - ./conf/mapred-site.xml:/home/hadoop/hadoop-3.3.1/etc/hadoop/mapred-site.xml
      - ./conf/yarn-site.xml:/home/hadoop/hadoop-3.3.1/etc/hadoop/yarn-site.xml
      - ./spark-defaults.conf:/opt/spark-3.1.2-bin-without-hadoop/conf/spark-defaults.conf
    command:
      - "-c"
      - |
        echo "Starting ssh service..."
        service ssh start

        echo "Formating the filesystem..."
        hdfs namenode -format
        
        echo "Starting NameNode, DataNode, Resource Manager and Node Manager..."
        hdfs --daemon start namenode
        hdfs --daemon start datanode
        yarn --daemon start resourcemanager
        yarn --daemon start nodemanager
        yarn --daemon start proxyserver

        echo "Creating the HDFS directories required to execute MapReduce jobs..."
        hdfs dfs -mkdir /user
        hdfs dfs -mkdir /user/$$(whoami)
        hdfs dfs -mkdir /spark-logs

        echo "Starting spark..."
        export SPARK_DIST_CLASSPATH=$$(hadoop classpath)
        /opt/spark-3.1.2-bin-without-hadoop/sbin/start-all.sh
        /opt/spark-3.1.2-bin-without-hadoop/sbin/start-history-server.sh

        tail -f /home/hadoop/hadoop-3.3.1/logs/*log
