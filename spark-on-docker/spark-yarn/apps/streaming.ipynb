{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complicated-harmony",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-24 01:00:35,983 WARN util.Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 172.22.0.5 instead (on interface eth0)\n",
      "2021-10-24 01:00:35,983 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.2.0-bin-without-hadoop/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-68595fb9-9853-4183-9dc4-09da75ac10d5;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.2.0-bin-without-hadoop/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.0/spark-sql-kafka-0-10_2.12-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0!spark-sql-kafka-0-10_2.12.jar (507ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.2.0/spark-token-provider-kafka-0-10_2.12-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0!spark-token-provider-kafka-0-10_2.12.jar (192ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.0/kafka-clients-2.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.0!kafka-clients.jar (871ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (250ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.1/hadoop-client-runtime-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.1!hadoop-client-runtime.jar (3310ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (348ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.1/hadoop-client-api-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.1!hadoop-client-api.jar (2131ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.htrace#htrace-core4;4.1.0-incubating!htrace-core4.jar (309ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (244ms)\n",
      ":: resolution report :: resolve 24318ms :: artifacts dl 8173ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   9   |   9   |   0   ||   13  |   9   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-68595fb9-9853-4183-9dc4-09da75ac10d5\n",
      "\tconfs: [default]\n",
      "\t9 artifacts copied, 4 already retrieved (58377kB/35ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2021-10-24 01:01:10,207 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2021-10-24 01:01:14,760 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.0.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,760 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.0.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,760 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,761 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,761 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,761 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,761 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,761 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,761 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,761 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,761 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,761 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar added multiple times to distributed cache.\n",
      "2021-10-24 01:01:14,761 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar added multiple times to distributed cache.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc0403f4e80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "config = SparkConf() \\\n",
    "                    .setAppName('Spark Streaming') \\\n",
    "                    .set(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\") \\\n",
    "                    .set(\"spark.sql.streaming.checkpointLocation\", \"hdfs:///user/root/checkpoint\")\n",
    "\n",
    "sc = SparkContext(conf=config)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55998c-2fb7-406d-aaa1-31c3d0eb049a",
   "metadata": {},
   "source": [
    "### Import some spark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71dc4e2f-8bc4-4551-8d2c-355861f8e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-nebraska",
   "metadata": {},
   "source": [
    "## Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be4b7f-5389-40ca-8737-fdb3396e5ec0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Streaming Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eba83a9-c478-4212-adf8-898d7d147563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to 1 topic, with headers\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka-broker:29092\") \\\n",
    "    .option(\"subscribe\", \"quickstart-events\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"minPartitions\", \"10\") \\\n",
    "    .option(\"failOnDataLoss\", \"true\") \\\n",
    "    .option(\"includeHeaders\", \"true\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a1422-6864-4de8-853f-57fcba974e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to a pattern\n",
    "df = spark \\\n",
    "     .readStream \\\n",
    "     .format(\"kafka\") \\\n",
    "     .option(\"kafka.bootstrap.servers\", \"kafka-broker:29092\") \\\n",
    "     .option(\"subscribePattern\", \"topic.*\") \\\n",
    "     .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c06eeb-3d22-4529-a97a-a80f40227b49",
   "metadata": {},
   "source": [
    "#### Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f549f49-ebcf-4563-8bed-dcfdebb124ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\n",
    "df = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"headers\") \\\n",
    "  .withColumn(\"value\", lit(\"changing events value\")) \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-broker:29092\") \\\n",
    "  .option(\"topic\", \"another-topic\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea1e6f-2b0d-4a62-95be-aef5fe2547f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write key-value data from a DataFrame to Kafka using a topic specified in the data\n",
    "df = df \\\n",
    "  .selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .withColumn(\"value\", lit(\"outro valor\")) \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-broker:29092\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e2b68-8062-4239-a979-0127391273b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Batch Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wooden-newark",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------+\n",
      "| key|               value|headers|\n",
      "+----+--------------------+-------+\n",
      "|null|            testando|   null|\n",
      "|null|          novo texto|   null|\n",
      "|null|       aqui vai dado|   null|\n",
      "|null|                  oi|   null|\n",
      "|null|                bora|   null|\n",
      "|null|                 vai|   null|\n",
      "|null|          tudo novo?|   null|\n",
      "|null|           ola,mundo|   null|\n",
      "|null|             eu,jose|   null|\n",
      "|null|         aqui,denovo|   null|\n",
      "|null|E la vamos nos de...|   null|\n",
      "|null|    testando mais um|   null|\n",
      "|null|                e ai|   null|\n",
      "|null|                 foi|   null|\n",
      "|null|            e agora?|   null|\n",
      "|null|            so isto?|   null|\n",
      "+----+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subscribe to 1 topic, with headers\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-broker:29092\") \\\n",
    "  .option(\"subscribe\", \"quickstart-events\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"headers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b224b57c-1be4-4fd8-96ee-3b20f2018f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to multiple topics, specifying explicit Kafka offsets\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-broker:29092\") \\\n",
    "  .option(\"subscribe\", \"topic1,topic2\") \\\n",
    "  .option(\"startingOffsets\", \"\"\"{\"topic1\":{\"0\":23,\"1\":-2},\"topic2\":{\"0\":-2}}\"\"\") \\\n",
    "  .option(\"endingOffsets\", \"\"\"{\"topic1\":{\"0\":50,\"1\":-1},\"topic2\":{\"0\":-1}}\"\"\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0128b9-0e72-4e87-8a7c-9e98604c87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to a pattern, at the earliest and latest offsets\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-broker:29092\") \\\n",
    "  .option(\"subscribePattern\", \"topic.*\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a7522a6-994b-4da7-af96-c312e07d4fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      " |-- headers: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33681a6e-1551-4c02-a208-d17bbb309592",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"value\", lit(\"some text\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34344d28-ea70-444a-838a-ec17bb5b77f1",
   "metadata": {},
   "source": [
    "#### Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c0b069-8041-4c9a-a467-659525d9e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .write \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-broker:29092\") \\\n",
    "  .option(\"topic\", \"another-topic\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb7861-7f42-404f-8458-a9132c99b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write key-value data from a DataFrame to Kafka using a topic specified in the data\n",
    "df.selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .write \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-broker:29092\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739cb97f-33f6-447a-b888-0ddc166a98b7",
   "metadata": {},
   "source": [
    "## Spark Streaming (DStreams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd149a8-17e8-4028-a1c4-be74470449b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
