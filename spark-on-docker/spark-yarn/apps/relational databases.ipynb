{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e26607e-1429-42e5-b2c1-ca4c8ddfaffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 01:46:27,305 WARN util.Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 172.18.0.3 instead (on interface eth0)\n",
      "2023-08-31 01:46:27,306 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "mysql#mysql-connector-java added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-67b1b491-14c6-4f83-ba6c-6efa55715028;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.4.1-bin-without-hadoop/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound mysql#mysql-connector-java;8.0.33 in central\n",
      "\tfound com.mysql#mysql-connector-j;8.0.33 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      "\tfound org.postgresql#postgresql;42.6.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      ":: resolution report :: resolve 103ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;8.0.33 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.33 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.6.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-67b1b491-14c6-4f83-ba6c-6efa55715028\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "2023-08-31 01:46:27,759 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-08-31 01:46:27,964 INFO spark.SparkContext: Running Spark version 3.4.1\n",
      "2023-08-31 01:46:27,978 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-08-31 01:46:27,978 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-08-31 01:46:27,978 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-08-31 01:46:27,978 INFO spark.SparkContext: Submitted application: Connect to Relational databases\n",
      "2023-08-31 01:46:27,990 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-08-31 01:46:27,998 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-08-31 01:46:27,999 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-08-31 01:46:28,031 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2023-08-31 01:46:28,031 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2023-08-31 01:46:28,031 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-08-31 01:46:28,031 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-08-31 01:46:28,031 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "2023-08-31 01:46:28,197 INFO util.Utils: Successfully started service 'sparkDriver' on port 46217.\n",
      "2023-08-31 01:46:28,260 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-08-31 01:46:28,308 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-08-31 01:46:28,322 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-08-31 01:46:28,323 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-08-31 01:46:28,359 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-08-31 01:46:28,372 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-d295e622-656d-49d3-aeb9-51d6bcbb3ff6\n",
      "2023-08-31 01:46:28,382 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "2023-08-31 01:46:28,422 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-08-31 01:46:28,452 INFO util.log: Logging initialized @1811ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-08-31 01:46:28,508 INFO ui.JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "2023-08-31 01:46:28,514 INFO server.Server: jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.16+8\n",
      "2023-08-31 01:46:28,523 INFO server.Server: Started @1883ms\n",
      "2023-08-31 01:46:28,539 INFO server.AbstractConnector: Started ServerConnector@101afc5c{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-08-31 01:46:28,539 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-08-31 01:46:28,549 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@672f5d52{/,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:28,796 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-08-31 01:46:29,238 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-08-31 01:46:29,239 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-08-31 01:46:29,246 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
      "2023-08-31 01:46:29,246 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2023-08-31 01:46:29,247 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2023-08-31 01:46:29,248 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2023-08-31 01:46:29,250 INFO yarn.Client: Preparing resources for our AM container\n",
      "2023-08-31 01:46:29,282 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2023-08-31 01:46:30,371 INFO yarn.Client: Uploading resource file:/tmp/spark-af2a5be5-a3e0-4017-b71c-44ac8bf10d6d/__spark_libs__1916558040134499756.zip -> hdfs://localhost:9000/user/root/.sparkStaging/application_1693445876092_0003/__spark_libs__1916558040134499756.zip\n",
      "2023-08-31 01:46:31,077 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.postgresql_postgresql-42.6.0.jar -> hdfs://localhost:9000/user/root/.sparkStaging/application_1693445876092_0003/org.postgresql_postgresql-42.6.0.jar\n",
      "2023-08-31 01:46:31,509 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.mysql_mysql-connector-j-8.0.33.jar -> hdfs://localhost:9000/user/root/.sparkStaging/application_1693445876092_0003/com.mysql_mysql-connector-j-8.0.33.jar\n",
      "2023-08-31 01:46:31,545 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.google.protobuf_protobuf-java-3.21.9.jar -> hdfs://localhost:9000/user/root/.sparkStaging/application_1693445876092_0003/com.google.protobuf_protobuf-java-3.21.9.jar\n",
      "2023-08-31 01:46:31,564 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.checkerframework_checker-qual-3.31.0.jar -> hdfs://localhost:9000/user/root/.sparkStaging/application_1693445876092_0003/org.checkerframework_checker-qual-3.31.0.jar\n",
      "2023-08-31 01:46:31,582 INFO yarn.Client: Uploading resource file:/opt/spark-3.4.1-bin-without-hadoop/python/lib/pyspark.zip -> hdfs://localhost:9000/user/root/.sparkStaging/application_1693445876092_0003/pyspark.zip\n",
      "2023-08-31 01:46:32,022 INFO yarn.Client: Uploading resource file:/opt/spark-3.4.1-bin-without-hadoop/python/lib/py4j-0.10.9.7-src.zip -> hdfs://localhost:9000/user/root/.sparkStaging/application_1693445876092_0003/py4j-0.10.9.7-src.zip\n",
      "2023-08-31 01:46:32,480 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.postgresql_postgresql-42.6.0.jar added multiple times to distributed cache.\n",
      "2023-08-31 01:46:32,480 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/com.mysql_mysql-connector-j-8.0.33.jar added multiple times to distributed cache.\n",
      "2023-08-31 01:46:32,480 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-3.21.9.jar added multiple times to distributed cache.\n",
      "2023-08-31 01:46:32,480 WARN yarn.Client: Same path resource file:///root/.ivy2/jars/org.checkerframework_checker-qual-3.31.0.jar added multiple times to distributed cache.\n",
      "2023-08-31 01:46:32,580 INFO yarn.Client: Uploading resource file:/tmp/spark-af2a5be5-a3e0-4017-b71c-44ac8bf10d6d/__spark_conf__8475497974094752173.zip -> hdfs://localhost:9000/user/root/.sparkStaging/application_1693445876092_0003/__spark_conf__.zip\n",
      "2023-08-31 01:46:33,044 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2023-08-31 01:46:33,044 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2023-08-31 01:46:33,044 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-08-31 01:46:33,044 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-08-31 01:46:33,044 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "2023-08-31 01:46:33,068 INFO yarn.Client: Submitting application application_1693445876092_0003 to ResourceManager\n",
      "2023-08-31 01:46:33,101 INFO impl.YarnClientImpl: Submitted application application_1693445876092_0003\n",
      "2023-08-31 01:46:34,103 INFO yarn.Client: Application report for application_1693445876092_0003 (state: ACCEPTED)\n",
      "2023-08-31 01:46:34,105 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1693446393081\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://localhost:8088/proxy/application_1693445876092_0003/\n",
      "\t user: root\n",
      "2023-08-31 01:46:35,108 INFO yarn.Client: Application report for application_1693445876092_0003 (state: ACCEPTED)\n",
      "2023-08-31 01:46:36,109 INFO yarn.Client: Application report for application_1693445876092_0003 (state: ACCEPTED)\n",
      "2023-08-31 01:46:36,547 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> localhost, PROXY_URI_BASES -> http://localhost:8088/proxy/application_1693445876092_0003), /proxy/application_1693445876092_0003\n",
      "2023-08-31 01:46:36,865 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "2023-08-31 01:46:37,113 INFO yarn.Client: Application report for application_1693445876092_0003 (state: RUNNING)\n",
      "2023-08-31 01:46:37,113 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.18.0.3\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1693446393081\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://localhost:8088/proxy/application_1693445876092_0003/\n",
      "\t user: root\n",
      "2023-08-31 01:46:37,114 INFO cluster.YarnClientSchedulerBackend: Application application_1693445876092_0003 has started running.\n",
      "2023-08-31 01:46:37,120 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38259.\n",
      "2023-08-31 01:46:37,120 INFO netty.NettyBlockTransferService: Server created on localhost:38259\n",
      "2023-08-31 01:46:37,122 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-08-31 01:46:37,139 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 38259, None)\n",
      "2023-08-31 01:46:37,177 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:38259 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 38259, None)\n",
      "2023-08-31 01:46:37,183 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 38259, None)\n",
      "2023-08-31 01:46:37,184 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 38259, None)\n",
      "2023-08-31 01:46:37,333 INFO history.SingleEventLogFileWriter: Logging events to hdfs://localhost:9000/spark-logs/application_1693445876092_0003.inprogress\n",
      "2023-08-31 01:46:37,430 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@672f5d52{/,null,STOPPED,@Spark}\n",
      "2023-08-31 01:46:37,430 INFO ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,438 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@50b3829{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,438 INFO ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,439 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65afb9b0{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,439 INFO ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,440 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a784fed{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,440 INFO ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,440 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a04ab3a{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,440 INFO ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,441 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35813e5c{/stages,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,441 INFO ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,441 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67215343{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,441 INFO ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,442 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fb6890{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,442 INFO ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,443 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2da54dd6{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,444 INFO ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,444 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c0d4b5b{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,445 INFO ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,445 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23d98b81{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,445 INFO ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,446 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f10bde{/storage,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,446 INFO ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,447 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3476f11d{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,447 INFO ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,448 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2dcaa8c3{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,448 INFO ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,448 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e0f8353{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,448 INFO ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,449 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@dae544a{/environment,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,449 INFO ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,450 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6daba6c0{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,450 INFO ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,450 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4446af4{/executors,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,450 INFO ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,451 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ade4fea{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,451 INFO ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,452 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4bfcac45{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,452 INFO ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,453 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36966799{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,453 INFO ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,459 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cf5b0fd{/static,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,459 INFO ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,460 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ad670b1{/,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,460 INFO ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,461 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3056b0a7{/api,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,462 INFO ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,462 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@370dfb9b{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,463 INFO ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,463 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c82c93c{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:37,465 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:37,466 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@78764798{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:39,452 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:46232) with ID 1,  ResourceProfileId 0\n",
      "2023-08-31 01:46:39,531 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:41785 with 434.4 MiB RAM, BlockManagerId(1, localhost, 41785, None)\n",
      "2023-08-31 01:46:41,819 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:46254) with ID 2,  ResourceProfileId 0\n",
      "2023-08-31 01:46:41,890 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:42505 with 434.4 MiB RAM, BlockManagerId(2, localhost, 42505, None)\n",
      "2023-08-31 01:46:41,897 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2023-08-31 01:46:42,084 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-08-31 01:46:42,086 INFO internal.SharedState: Warehouse path is 'file:/opt/apps/spark-warehouse'.\n",
      "2023-08-31 01:46:42,095 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:42,097 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@433d421f{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:42,098 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:42,098 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e0f493e{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:42,098 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:42,099 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@698fc98e{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:42,099 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:42,099 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64da8f7e{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-08-31 01:46:42,100 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-08-31 01:46:42,101 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7fcc5093{/static/sql,null,AVAILABLE,@Spark}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Connect to Relational databases</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff9c141820>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "config = SparkConf() \\\n",
    "                    .setAppName('Connect to Relational databases') \\\n",
    "                    .set(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.33,org.postgresql:postgresql:42.6.0\")\n",
    "\n",
    "sc = SparkContext(conf=config)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57439da-f6ba-4332-8680-0dca0d896390",
   "metadata": {},
   "source": [
    "# MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "093c11c7-0133-439b-b5c2-8ef2079eb579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 01:47:59,870 INFO codegen.CodeGenerator: Code generated in 123.672084 ms\n",
      "2023-08-31 01:47:59,904 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-08-31 01:47:59,914 INFO scheduler.DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-08-31 01:47:59,914 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-08-31 01:47:59,914 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-08-31 01:47:59,915 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-08-31 01:47:59,917 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-08-31 01:47:59,983 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.7 KiB, free 434.4 MiB)\n",
      "2023-08-31 01:48:00,012 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.4 MiB)\n",
      "2023-08-31 01:48:00,015 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:38259 (size: 6.5 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:00,018 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
      "2023-08-31 01:48:00,035 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-08-31 01:48:00,036 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2023-08-31 01:48:00,070 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) \n",
      "2023-08-31 01:48:00,268 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:41785 (size: 6.5 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:01,126 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1066 ms on localhost (executor 1) (1/1)\n",
      "2023-08-31 01:48:01,128 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-08-31 01:48:01,131 INFO scheduler.DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 1.206 s\n",
      "2023-08-31 01:48:01,133 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-08-31 01:48:01,133 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "2023-08-31 01:48:01,134 INFO scheduler.DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 1.230084 s\n",
      "2023-08-31 01:48:01,162 INFO codegen.CodeGenerator: Code generated in 12.245792 ms\n",
      "2023-08-31 01:48:01,288 INFO codegen.CodeGenerator: Code generated in 9.198125 ms\n",
      "2023-08-31 01:48:01,310 INFO scheduler.DAGScheduler: Registering RDD 5 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "2023-08-31 01:48:01,316 INFO scheduler.DAGScheduler: Got map stage job 1 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-08-31 01:48:01,316 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0)\n",
      "2023-08-31 01:48:01,316 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-08-31 01:48:01,317 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-08-31 01:48:01,318 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------+-------------------+\n",
      "|actor_id|first_name|   last_name|        last_update|\n",
      "+--------+----------+------------+-------------------+\n",
      "|       1|  PENELOPE|     GUINESS|2006-02-15 04:34:33|\n",
      "|       2|      NICK|    WAHLBERG|2006-02-15 04:34:33|\n",
      "|       3|        ED|       CHASE|2006-02-15 04:34:33|\n",
      "|       4|  JENNIFER|       DAVIS|2006-02-15 04:34:33|\n",
      "|       5|    JOHNNY|LOLLOBRIGIDA|2006-02-15 04:34:33|\n",
      "|       6|     BETTE|   NICHOLSON|2006-02-15 04:34:33|\n",
      "|       7|     GRACE|      MOSTEL|2006-02-15 04:34:33|\n",
      "|       8|   MATTHEW|   JOHANSSON|2006-02-15 04:34:33|\n",
      "|       9|       JOE|       SWANK|2006-02-15 04:34:33|\n",
      "|      10| CHRISTIAN|       GABLE|2006-02-15 04:34:33|\n",
      "|      11|      ZERO|        CAGE|2006-02-15 04:34:33|\n",
      "|      12|      KARL|       BERRY|2006-02-15 04:34:33|\n",
      "|      13|       UMA|        WOOD|2006-02-15 04:34:33|\n",
      "|      14|    VIVIEN|      BERGEN|2006-02-15 04:34:33|\n",
      "|      15|      CUBA|     OLIVIER|2006-02-15 04:34:33|\n",
      "|      16|      FRED|     COSTNER|2006-02-15 04:34:33|\n",
      "|      17|     HELEN|      VOIGHT|2006-02-15 04:34:33|\n",
      "|      18|       DAN|        TORN|2006-02-15 04:34:33|\n",
      "|      19|       BOB|     FAWCETT|2006-02-15 04:34:33|\n",
      "|      20|   LUCILLE|       TRACY|2006-02-15 04:34:33|\n",
      "+--------+----------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 01:48:01,336 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 434.4 MiB)\n",
      "2023-08-31 01:48:01,339 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 434.4 MiB)\n",
      "2023-08-31 01:48:01,339 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:38259 (size: 7.1 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:01,340 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
      "2023-08-31 01:48:01,341 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-08-31 01:48:01,341 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "2023-08-31 01:48:01,343 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor 1, partition 0, PROCESS_LOCAL, 7209 bytes) \n",
      "2023-08-31 01:48:01,355 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:41785 (size: 7.1 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:01,475 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 133 ms on localhost (executor 1) (1/1)\n",
      "2023-08-31 01:48:01,475 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2023-08-31 01:48:01,476 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.153 s\n",
      "2023-08-31 01:48:01,477 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-08-31 01:48:01,477 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-08-31 01:48:01,477 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-08-31 01:48:01,477 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-08-31 01:48:01,498 INFO codegen.CodeGenerator: Code generated in 5.799167 ms\n",
      "2023-08-31 01:48:01,511 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "2023-08-31 01:48:01,513 INFO scheduler.DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-08-31 01:48:01,513 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
      "2023-08-31 01:48:01,513 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "2023-08-31 01:48:01,513 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-08-31 01:48:01,513 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-08-31 01:48:01,519 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.2 KiB, free 434.3 MiB)\n",
      "2023-08-31 01:48:01,522 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.3 MiB)\n",
      "2023-08-31 01:48:01,522 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:38259 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:01,522 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
      "2023-08-31 01:48:01,523 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-08-31 01:48:01,523 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "2023-08-31 01:48:01,527 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (localhost, executor 2, partition 0, NODE_LOCAL, 7374 bytes) \n",
      "2023-08-31 01:48:01,687 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:42505 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:01,829 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 127.0.0.1:46254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- last_update: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 01:48:02,104 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 578 ms on localhost (executor 2) (1/1)\n",
      "2023-08-31 01:48:02,104 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2023-08-31 01:48:02,105 INFO scheduler.DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.587 s\n",
      "2023-08-31 01:48:02,105 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-08-31 01:48:02,105 INFO cluster.YarnScheduler: Killing all running tasks in stage 3: Stage finished\n",
      "2023-08-31 01:48:02,106 INFO scheduler.DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 0.594104 s\n",
      "2023-08-31 01:48:13,073 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on localhost:38259 in memory (size: 7.1 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:13,074 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on localhost:41785 in memory (size: 7.1 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:13,082 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on localhost:38259 in memory (size: 6.5 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:13,084 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on localhost:41785 in memory (size: 6.5 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:13,089 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on localhost:38259 in memory (size: 5.9 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:13,093 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on localhost:42505 in memory (size: 5.9 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Setup the JDBC connection\n",
    "jdbc_url = \"jdbc:mysql://mysql:3306/sakila\"\n",
    "connection_properties = {\n",
    "      \"user\" : \"root\",\n",
    "      \"password\" : \"root\",\n",
    "      \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "\n",
    "# Create a query\n",
    "query = \"\"\"\n",
    "            SELECT * FROM actor\n",
    "        \"\"\"\n",
    "\n",
    "# run the query\n",
    "bh_bairros_df = spark.read \\\n",
    "                     .jdbc(url=jdbc_url, \n",
    "                           table=f\"({query}) AS t\", \n",
    "                           properties=connection_properties)\n",
    "\n",
    "bh_bairros_df.show()\n",
    "print(bh_bairros_df.count())\n",
    "bh_bairros_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0352cde0-f10d-40bd-a7d7-c2a3e8b61beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 01:48:24,393 INFO codegen.CodeGenerator: Code generated in 8.297542 ms\n",
      "2023-08-31 01:48:24,448 INFO spark.SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "2023-08-31 01:48:24,449 INFO scheduler.DAGScheduler: Got job 3 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-08-31 01:48:24,449 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "2023-08-31 01:48:24,449 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-08-31 01:48:24,449 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-08-31 01:48:24,451 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-08-31 01:48:24,471 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 30.0 KiB, free 434.4 MiB)\n",
      "2023-08-31 01:48:24,474 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 434.4 MiB)\n",
      "2023-08-31 01:48:24,475 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:38259 (size: 13.6 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:48:24,475 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
      "2023-08-31 01:48:24,476 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-08-31 01:48:24,476 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "2023-08-31 01:48:24,477 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (localhost, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) \n",
      "2023-08-31 01:48:24,505 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:41785 (size: 13.6 KiB, free: 434.4 MiB)\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- last_update: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 01:48:25,495 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 1017 ms on localhost (executor 1) (1/1)\n",
      "2023-08-31 01:48:25,495 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2023-08-31 01:48:25,496 INFO scheduler.DAGScheduler: ResultStage 4 (jdbc at NativeMethodAccessorImpl.java:0) finished in 1.044 s\n",
      "2023-08-31 01:48:25,496 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-08-31 01:48:25,496 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\n",
      "2023-08-31 01:48:25,496 INFO scheduler.DAGScheduler: Job 3 finished: jdbc at NativeMethodAccessorImpl.java:0, took 1.048105 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "bh_bairros_df.write \\\n",
    "             .mode(\"overwrite\") \\\n",
    "             .option(\"truncate\", \"true\") \\\n",
    "             .jdbc(url=jdbc_url, \n",
    "                   table=\"new_table\",\n",
    "                   properties=connection_properties)\n",
    "\n",
    "bh_bairros_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538f507-1214-43ea-bd8a-7c1fc1d6f659",
   "metadata": {},
   "source": [
    "# Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a43e76b-ee63-4137-8d16-de9cb121ba1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 01:52:46,495 INFO codegen.CodeGenerator: Code generated in 13.945333 ms\n",
      "2023-08-31 01:52:46,504 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-08-31 01:52:46,505 INFO scheduler.DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-08-31 01:52:46,505 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-08-31 01:52:46,505 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-08-31 01:52:46,505 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-08-31 01:52:46,506 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-08-31 01:52:46,516 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.8 KiB, free 434.3 MiB)\n",
      "2023-08-31 01:52:46,519 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.3 MiB)\n",
      "2023-08-31 01:52:46,520 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:38259 (size: 6.3 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:52:46,520 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n",
      "2023-08-31 01:52:46,521 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-08-31 01:52:46,521 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "2023-08-31 01:52:46,522 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (localhost, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) \n",
      "2023-08-31 01:52:46,548 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:41785 (size: 6.3 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:52:46,752 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 230 ms on localhost (executor 1) (1/1)\n",
      "2023-08-31 01:52:46,752 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2023-08-31 01:52:46,753 INFO scheduler.DAGScheduler: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 0.246 s\n",
      "2023-08-31 01:52:46,754 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-08-31 01:52:46,754 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\n",
      "2023-08-31 01:52:46,754 INFO scheduler.DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.249608 s\n",
      "2023-08-31 01:52:46,768 INFO codegen.CodeGenerator: Code generated in 10.117083 ms\n",
      "2023-08-31 01:52:46,792 INFO scheduler.DAGScheduler: Registering RDD 19 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "2023-08-31 01:52:46,792 INFO scheduler.DAGScheduler: Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-08-31 01:52:46,792 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0)\n",
      "2023-08-31 01:52:46,792 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-08-31 01:52:46,792 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-08-31 01:52:46,793 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-08-31 01:52:46,796 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 13.5 KiB, free 434.3 MiB)\n",
      "2023-08-31 01:52:46,797 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 434.3 MiB)\n",
      "2023-08-31 01:52:46,798 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:38259 (size: 7.1 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:52:46,798 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
      "2023-08-31 01:52:46,799 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-08-31 01:52:46,799 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "2023-08-31 01:52:46,799 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (localhost, executor 2, partition 0, PROCESS_LOCAL, 7209 bytes) \n",
      "2023-08-31 01:52:46,832 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:42505 (size: 7.1 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+------+--------------------+\n",
      "|                  id|              name|              amount| price|       id_categories|\n",
      "+--------------------+------------------+--------------------+------+--------------------+\n",
      "|1.000000000000000000|         Lampshade|100.0000000000000...|800.00|4.000000000000000000|\n",
      "|2.000000000000000000|Table for painting|1000.000000000000...|560.00|9.000000000000000000|\n",
      "|3.000000000000000000|     Notebook desk|10000.00000000000...| 25.50|9.000000000000000000|\n",
      "|4.000000000000000000|     Computer desk|350.0000000000000...|320.50|6.000000000000000000|\n",
      "|5.000000000000000000|             Chair|3000.000000000000...|210.64|9.000000000000000000|\n",
      "|6.000000000000000000|        Home alarm|750.0000000000000...|460.00|4.000000000000000000|\n",
      "+--------------------+------------------+--------------------+------+--------------------+\n",
      "\n",
      "6\n",
      "root\n",
      " |-- id: decimal(38,18) (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- amount: decimal(38,18) (nullable = true)\n",
      " |-- price: decimal(7,2) (nullable = true)\n",
      " |-- id_categories: decimal(38,18) (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 01:52:47,161 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 362 ms on localhost (executor 2) (1/1)\n",
      "2023-08-31 01:52:47,161 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "2023-08-31 01:52:47,162 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.368 s\n",
      "2023-08-31 01:52:47,162 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-08-31 01:52:47,162 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-08-31 01:52:47,162 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-08-31 01:52:47,162 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-08-31 01:52:47,177 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "2023-08-31 01:52:47,178 INFO scheduler.DAGScheduler: Got job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-08-31 01:52:47,178 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (count at NativeMethodAccessorImpl.java:0)\n",
      "2023-08-31 01:52:47,178 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "2023-08-31 01:52:47,179 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-08-31 01:52:47,179 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[22] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-08-31 01:52:47,182 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.2 KiB, free 434.3 MiB)\n",
      "2023-08-31 01:52:47,185 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.3 MiB)\n",
      "2023-08-31 01:52:47,185 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:38259 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:52:47,186 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n",
      "2023-08-31 01:52:47,186 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[22] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-08-31 01:52:47,186 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "2023-08-31 01:52:47,187 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (localhost, executor 1, partition 0, NODE_LOCAL, 7374 bytes) \n",
      "2023-08-31 01:52:47,200 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:41785 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:52:47,211 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 127.0.0.1:46232\n",
      "2023-08-31 01:52:47,287 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 100 ms on localhost (executor 1) (1/1)\n",
      "2023-08-31 01:52:47,287 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "2023-08-31 01:52:47,288 INFO scheduler.DAGScheduler: ResultStage 8 (count at NativeMethodAccessorImpl.java:0) finished in 0.108 s\n",
      "2023-08-31 01:52:47,288 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-08-31 01:52:47,288 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\n",
      "2023-08-31 01:52:47,288 INFO scheduler.DAGScheduler: Job 6 finished: count at NativeMethodAccessorImpl.java:0, took 0.110448 s\n",
      "2023-08-31 01:55:31,754 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on localhost:38259 in memory (size: 6.3 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:55:31,764 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on localhost:41785 in memory (size: 6.3 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:55:31,774 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on localhost:38259 in memory (size: 5.9 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:55:31,777 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on localhost:41785 in memory (size: 5.9 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:55:31,783 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on localhost:38259 in memory (size: 7.1 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:55:31,785 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on localhost:42505 in memory (size: 7.1 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:55:31,790 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on localhost:38259 in memory (size: 13.6 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:55:31,792 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on localhost:41785 in memory (size: 13.6 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Setup the JDBC connection\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/mydb\"\n",
    "connection_properties = {\n",
    "      \"user\" : \"user\",\n",
    "      \"password\" : \"pass\",\n",
    "      \"driver\" : \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "# Create a query\n",
    "query = \"\"\"\n",
    "            SELECT * FROM products\n",
    "        \"\"\"\n",
    "\n",
    "# run the query\n",
    "df = spark.read \\\n",
    "                     .jdbc(url=jdbc_url, \n",
    "                           table=f\"({query}) AS t\", \n",
    "                           properties=connection_properties)\n",
    "\n",
    "df.show()\n",
    "print(df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e7b1dfe-6503-4b3b-ba90-037a0d598ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 01:59:27,611 INFO codegen.CodeGenerator: Code generated in 11.538208 ms\n",
      "2023-08-31 01:59:27,628 INFO spark.SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "2023-08-31 01:59:27,629 INFO scheduler.DAGScheduler: Got job 7 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-08-31 01:59:27,629 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "2023-08-31 01:59:27,630 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-08-31 01:59:27,630 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-08-31 01:59:27,631 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[27] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-08-31 01:59:27,643 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 30.1 KiB, free 434.4 MiB)\n",
      "2023-08-31 01:59:27,646 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 434.4 MiB)\n",
      "2023-08-31 01:59:27,648 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:38259 (size: 13.7 KiB, free: 434.4 MiB)\n",
      "2023-08-31 01:59:27,648 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
      "2023-08-31 01:59:27,649 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[27] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-08-31 01:59:27,649 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "2023-08-31 01:59:27,650 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (localhost, executor 1, partition 0, PROCESS_LOCAL, 7220 bytes) \n",
      "2023-08-31 01:59:27,698 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:41785 (size: 13.7 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: decimal(38,18) (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- amount: decimal(38,18) (nullable = true)\n",
      " |-- price: decimal(7,2) (nullable = true)\n",
      " |-- id_categories: decimal(38,18) (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 01:59:27,845 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 194 ms on localhost (executor 1) (1/1)\n",
      "2023-08-31 01:59:27,845 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "2023-08-31 01:59:27,845 INFO scheduler.DAGScheduler: ResultStage 9 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.214 s\n",
      "2023-08-31 01:59:27,846 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-08-31 01:59:27,846 INFO cluster.YarnScheduler: Killing all running tasks in stage 9: Stage finished\n",
      "2023-08-31 01:59:27,846 INFO scheduler.DAGScheduler: Job 7 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.217860 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.write \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .option(\"truncate\", \"true\") \\\n",
    "     .jdbc(url=jdbc_url, \n",
    "           table=\"new_table\",\n",
    "           properties=connection_properties)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4becb9-ea20-483a-a860-c95c24a5686b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
