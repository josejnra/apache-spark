{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4807307d-c64e-451a-b104-87ec5d06df95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 00:01:09,829 WARN util.Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 172.18.0.2 instead (on interface eth0)\n",
      "2023-04-29 00:01:09,830 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2023-04-29 00:01:10,305 INFO spark.SparkContext: Running Spark version 3.4.0\n",
      "2023-04-29 00:01:10,329 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-04-29 00:01:10,329 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-04-29 00:01:10,329 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-04-29 00:01:10,330 INFO spark.SparkContext: Submitted application: Read and Write\n",
      "2023-04-29 00:01:10,344 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-04-29 00:01:10,354 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-04-29 00:01:10,355 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-04-29 00:01:10,386 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2023-04-29 00:01:10,386 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2023-04-29 00:01:10,387 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-04-29 00:01:10,387 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-04-29 00:01:10,387 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "2023-04-29 00:01:10,525 INFO util.Utils: Successfully started service 'sparkDriver' on port 42355.\n",
      "2023-04-29 00:01:10,541 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-04-29 00:01:10,562 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-04-29 00:01:10,575 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-04-29 00:01:10,575 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-04-29 00:01:10,603 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-04-29 00:01:10,621 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-6f33ad45-b539-429e-880d-a22b367185f8\n",
      "2023-04-29 00:01:10,632 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "2023-04-29 00:01:10,666 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-04-29 00:01:10,703 INFO util.log: Logging initialized @1530ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-04-29 00:01:10,766 INFO ui.JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "2023-04-29 00:01:10,773 INFO server.Server: jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 11.0.12+7\n",
      "2023-04-29 00:01:10,785 INFO server.Server: Started @1612ms\n",
      "2023-04-29 00:01:10,810 INFO server.AbstractConnector: Started ServerConnector@7cb8d708{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-04-29 00:01:10,810 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-04-29 00:01:10,824 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@771285d9{/,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:10,838 INFO spark.SparkContext: Added JAR /opt/scala-project_2.12-0.1.0-SNAPSHOT.jar at spark://localhost:42355/jars/scala-project_2.12-0.1.0-SNAPSHOT.jar with timestamp 1682726470300\n",
      "2023-04-29 00:01:11,119 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2023-04-29 00:01:11,557 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-04-29 00:01:11,557 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-04-29 00:01:11,566 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
      "2023-04-29 00:01:11,566 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2023-04-29 00:01:11,566 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2023-04-29 00:01:11,568 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2023-04-29 00:01:11,571 INFO yarn.Client: Preparing resources for our AM container\n",
      "2023-04-29 00:01:11,596 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2023-04-29 00:01:12,425 INFO yarn.Client: Uploading resource file:/tmp/spark-d70754aa-ea11-4451-8de4-b652c96bbb40/__spark_libs__4740036928003885027.zip -> hdfs://localhost:9000/user/root/.sparkStaging/application_1682380615897_0015/__spark_libs__4740036928003885027.zip\n",
      "2023-04-29 00:01:12,799 INFO yarn.Client: Uploading resource file:/opt/scala-project_2.12-0.1.0-SNAPSHOT.jar -> hdfs://localhost:9000/user/root/.sparkStaging/application_1682380615897_0015/scala-project_2.12-0.1.0-SNAPSHOT.jar\n",
      "2023-04-29 00:01:12,815 INFO yarn.Client: Uploading resource file:/opt/spark-3.4.0-bin-without-hadoop/python/lib/pyspark.zip -> hdfs://localhost:9000/user/root/.sparkStaging/application_1682380615897_0015/pyspark.zip\n",
      "2023-04-29 00:01:12,829 INFO yarn.Client: Uploading resource file:/opt/spark-3.4.0-bin-without-hadoop/python/lib/py4j-0.10.9.7-src.zip -> hdfs://localhost:9000/user/root/.sparkStaging/application_1682380615897_0015/py4j-0.10.9.7-src.zip\n",
      "2023-04-29 00:01:12,953 INFO yarn.Client: Uploading resource file:/tmp/spark-d70754aa-ea11-4451-8de4-b652c96bbb40/__spark_conf__13398832719013376760.zip -> hdfs://localhost:9000/user/root/.sparkStaging/application_1682380615897_0015/__spark_conf__.zip\n",
      "2023-04-29 00:01:12,977 INFO spark.SecurityManager: Changing view acls to: root\n",
      "2023-04-29 00:01:12,978 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "2023-04-29 00:01:12,978 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-04-29 00:01:12,978 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-04-29 00:01:12,978 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "2023-04-29 00:01:12,993 INFO yarn.Client: Submitting application application_1682380615897_0015 to ResourceManager\n",
      "2023-04-29 00:01:13,022 INFO impl.YarnClientImpl: Submitted application application_1682380615897_0015\n",
      "2023-04-29 00:01:14,024 INFO yarn.Client: Application report for application_1682380615897_0015 (state: ACCEPTED)\n",
      "2023-04-29 00:01:14,026 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1682726473003\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://localhost:8088/proxy/application_1682380615897_0015/\n",
      "\t user: root\n",
      "2023-04-29 00:01:15,027 INFO yarn.Client: Application report for application_1682380615897_0015 (state: ACCEPTED)\n",
      "2023-04-29 00:01:15,801 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> localhost, PROXY_URI_BASES -> http://localhost:8088/proxy/application_1682380615897_0015), /proxy/application_1682380615897_0015\n",
      "2023-04-29 00:01:16,028 INFO yarn.Client: Application report for application_1682380615897_0015 (state: RUNNING)\n",
      "2023-04-29 00:01:16,028 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.18.0.2\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1682726473003\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://localhost:8088/proxy/application_1682380615897_0015/\n",
      "\t user: root\n",
      "2023-04-29 00:01:16,029 INFO cluster.YarnClientSchedulerBackend: Application application_1682380615897_0015 has started running.\n",
      "2023-04-29 00:01:16,034 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37111.\n",
      "2023-04-29 00:01:16,035 INFO netty.NettyBlockTransferService: Server created on localhost:37111\n",
      "2023-04-29 00:01:16,036 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-04-29 00:01:16,041 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 37111, None)\n",
      "2023-04-29 00:01:16,044 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:37111 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 37111, None)\n",
      "2023-04-29 00:01:16,045 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 37111, None)\n",
      "2023-04-29 00:01:16,046 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 37111, None)\n",
      "2023-04-29 00:01:16,156 INFO history.SingleEventLogFileWriter: Logging events to hdfs://localhost:9000/spark-logs/application_1682380615897_0015.inprogress\n",
      "2023-04-29 00:01:16,170 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "2023-04-29 00:01:16,284 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@771285d9{/,null,STOPPED,@Spark}\n",
      "2023-04-29 00:01:16,284 INFO ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,289 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@510b5f50{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,289 INFO ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,290 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d5f9348{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,291 INFO ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,291 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cc26bb3{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,291 INFO ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,292 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d42f608{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,292 INFO ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,292 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b0e8943{/stages,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,292 INFO ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,293 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a66de45{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,293 INFO ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,294 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27e7a4cd{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,294 INFO ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,295 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58ae88e2{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,295 INFO ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,296 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b705dd6{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,296 INFO ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,296 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f574976{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,296 INFO ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,297 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b0aae8c{/storage,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,297 INFO ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,298 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a004fb2{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,298 INFO ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,299 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b2f65b{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,299 INFO ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,299 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ba2a7f6{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,299 INFO ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,300 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@997d5e1{/environment,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,300 INFO ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,301 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35b5aa55{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,301 INFO ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,302 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3cc70f5f{/executors,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,302 INFO ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,302 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@77e6692c{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,303 INFO ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,304 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58293ee{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,304 INFO ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,304 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fbc28bd{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,304 INFO ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,311 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72ffe6f7{/static,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,311 INFO ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,312 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e4ab84a{/,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,312 INFO ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,313 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@689a2512{/api,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,314 INFO ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,314 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fe19c8{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,314 INFO ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,315 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4feb5eb1{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:16,318 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:16,319 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@574d3423{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:18,244 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:39792) with ID 1,  ResourceProfileId 0\n",
      "2023-04-29 00:01:18,336 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:39805 with 434.4 MiB RAM, BlockManagerId(1, localhost, 39805, None)\n",
      "2023-04-29 00:01:19,641 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:39812) with ID 2,  ResourceProfileId 0\n",
      "2023-04-29 00:01:19,723 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2023-04-29 00:01:19,726 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:39405 with 434.4 MiB RAM, BlockManagerId(2, localhost, 39405, None)\n",
      "2023-04-29 00:01:19,868 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-04-29 00:01:19,871 INFO internal.SharedState: Warehouse path is 'file:/opt/apps/spark-warehouse'.\n",
      "2023-04-29 00:01:19,882 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:19,883 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75683dd5{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:19,883 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:19,884 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a7af042{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:19,884 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:19,885 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@8a06fed{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:19,885 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:19,886 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4051ec67{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-04-29 00:01:19,886 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-29 00:01:19,887 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3639b9c8{/static/sql,null,AVAILABLE,@Spark}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Read and Write</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa28d3bce80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('Read and Write') \\\n",
    "                    .config(\"spark.jars\", \"/opt/scala-project_2.12-0.1.0-SNAPSHOT.jar\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40582db7-3ca5-4bcd-82a5-3f1cd305d09d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5427c23e-95b5-4edb-a15c-54c97770278b",
   "metadata": {},
   "source": [
    "## Create DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe59a86d-53b9-4f80-9682-83dd14dca23d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 00:01:21,900 INFO codegen.CodeGenerator: Code generated in 115.661801 ms\n",
      "2023-04-29 00:01:21,931 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-04-29 00:01:21,940 INFO scheduler.DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-04-29 00:01:21,941 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-04-29 00:01:21,941 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-04-29 00:01:21,942 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-04-29 00:01:21,944 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-04-29 00:01:22,000 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.6 KiB, free 434.4 MiB)\n",
      "2023-04-29 00:01:22,019 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.4 MiB)\n",
      "2023-04-29 00:01:22,021 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:37111 (size: 6.6 KiB, free: 434.4 MiB)\n",
      "2023-04-29 00:01:22,024 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
      "2023-04-29 00:01:22,033 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-04-29 00:01:22,033 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2023-04-29 00:01:22,056 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor 1, partition 0, PROCESS_LOCAL, 7391 bytes) \n",
      "2023-04-29 00:01:22,225 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:39805 (size: 6.6 KiB, free: 434.4 MiB)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name| id|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|Braga|  2|\n",
      "|Steve|  3|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 00:01:22,969 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 918 ms on localhost (executor 1) (1/1)\n",
      "2023-04-29 00:01:22,970 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-04-29 00:01:22,974 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 42847\n",
      "2023-04-29 00:01:22,976 INFO scheduler.DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 1.023 s\n",
      "2023-04-29 00:01:22,978 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-04-29 00:01:22,978 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "2023-04-29 00:01:22,979 INFO scheduler.DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 1.048188 s\n",
      "2023-04-29 00:01:22,989 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-04-29 00:01:22,990 INFO scheduler.DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-04-29 00:01:22,990 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-04-29 00:01:22,990 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-04-29 00:01:22,990 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-04-29 00:01:22,990 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-04-29 00:01:22,993 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.6 KiB, free 434.4 MiB)\n",
      "2023-04-29 00:01:22,995 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.4 MiB)\n",
      "2023-04-29 00:01:22,995 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:37111 (size: 6.6 KiB, free: 434.4 MiB)\n",
      "2023-04-29 00:01:22,996 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
      "2023-04-29 00:01:22,996 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
      "2023-04-29 00:01:22,996 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "2023-04-29 00:01:22,997 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor 1, partition 1, PROCESS_LOCAL, 7428 bytes) \n",
      "2023-04-29 00:01:23,011 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:39805 (size: 6.6 KiB, free: 434.4 MiB)\n",
      "2023-04-29 00:01:23,074 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 77 ms on localhost (executor 1) (1/1)\n",
      "2023-04-29 00:01:23,074 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2023-04-29 00:01:23,075 INFO scheduler.DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.084 s\n",
      "2023-04-29 00:01:23,075 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-04-29 00:01:23,075 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\n",
      "2023-04-29 00:01:23,075 INFO scheduler.DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.086492 s\n",
      "2023-04-29 00:01:23,099 INFO codegen.CodeGenerator: Code generated in 12.331036 ms\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import Row, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "row_list_data = [Row('Alice', 1), Row('Braga', 2), Row('Steve', 3)]\n",
    "df = spark.createDataFrame(row_list_data, ['name', 'id'])\n",
    "\n",
    "df = df.withColumn(\"id\", col(\"id\").cast(IntegerType()))\n",
    "    \n",
    "df.createOrReplaceTempView('users')\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686830b-3931-477c-87a6-3f1987f627fa",
   "metadata": {},
   "source": [
    "## Access via SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c95214-d157-495d-b7cf-c2f49f851396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaObject id=o70"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# register class that implements UDF interface\n",
    "spark.udf.registerJavaFunction(\"lowerCaseString\", \"com.myudfs.LowerCaseString\", returnType=StringType())\n",
    "\n",
    "# call register function from class MathUDFs that will register the udf in spark \n",
    "spark.sparkContext._jvm.com.myudfs.MathUDFs.registerUdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed856cf-a197-45bf-b586-73c261f43fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 00:01:23,294 INFO codegen.CodeGenerator: Code generated in 24.006056 ms\n",
      "2023-04-29 00:01:23,317 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-04-29 00:01:23,318 INFO scheduler.DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-04-29 00:01:23,318 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-04-29 00:01:23,318 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-04-29 00:01:23,318 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-04-29 00:01:23,318 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-04-29 00:01:23,323 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 25.9 KiB, free 434.3 MiB)\n",
      "2023-04-29 00:01:23,325 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 434.3 MiB)\n",
      "2023-04-29 00:01:23,326 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:37111 (size: 12.6 KiB, free: 434.4 MiB)\n",
      "2023-04-29 00:01:23,326 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n",
      "2023-04-29 00:01:23,327 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-04-29 00:01:23,327 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "2023-04-29 00:01:23,328 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (localhost, executor 1, partition 0, PROCESS_LOCAL, 7391 bytes) \n",
      "2023-04-29 00:01:23,340 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:39805 (size: 12.6 KiB, free: 434.4 MiB)\n",
      "2023-04-29 00:01:24,176 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 848 ms on localhost (executor 1) (1/1)\n",
      "2023-04-29 00:01:24,176 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2023-04-29 00:01:24,177 INFO scheduler.DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.857 s\n",
      "2023-04-29 00:01:24,177 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-04-29 00:01:24,177 INFO cluster.YarnScheduler: Killing all running tasks in stage 2: Stage finished\n",
      "2023-04-29 00:01:24,177 INFO scheduler.DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.860034 s\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|isGreaterThanZero(id)| name|\n",
      "+---------------------+-----+\n",
      "|                 true|Alice|\n",
      "|                 true|Braga|\n",
      "|                 true|Steve|\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 00:01:24,183 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-04-29 00:01:24,184 INFO scheduler.DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-04-29 00:01:24,184 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-04-29 00:01:24,184 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-04-29 00:01:24,184 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-04-29 00:01:24,184 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-04-29 00:01:24,187 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 25.9 KiB, free 434.3 MiB)\n",
      "2023-04-29 00:01:24,190 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 434.3 MiB)\n",
      "2023-04-29 00:01:24,190 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:37111 (size: 12.6 KiB, free: 434.4 MiB)\n",
      "2023-04-29 00:01:24,190 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
      "2023-04-29 00:01:24,191 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
      "2023-04-29 00:01:24,191 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "2023-04-29 00:01:24,192 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (localhost, executor 1, partition 1, PROCESS_LOCAL, 7428 bytes) \n",
      "2023-04-29 00:01:24,203 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:39805 (size: 12.6 KiB, free: 434.4 MiB)\n",
      "2023-04-29 00:01:24,260 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 68 ms on localhost (executor 1) (1/1)\n",
      "2023-04-29 00:01:24,260 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2023-04-29 00:01:24,262 INFO scheduler.DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.077 s\n",
      "2023-04-29 00:01:24,262 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-04-29 00:01:24,262 INFO cluster.YarnScheduler: Killing all running tasks in stage 3: Stage finished\n",
      "2023-04-29 00:01:24,262 INFO scheduler.DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.079129 s\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT isGreaterThanZero(id), name\n",
    "    FROM users\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba0bdc35-ccbc-452c-87ef-7a608e6ab248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 00:01:24,315 INFO codegen.CodeGenerator: Code generated in 12.17728 ms\n",
      "2023-04-29 00:01:24,323 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-04-29 00:01:24,324 INFO scheduler.DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-04-29 00:01:24,324 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-04-29 00:01:24,324 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-04-29 00:01:24,324 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-04-29 00:01:24,325 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-04-29 00:01:24,328 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.1 KiB, free 434.3 MiB)\n",
      "2023-04-29 00:01:24,330 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 434.3 MiB)\n",
      "2023-04-29 00:01:24,330 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:37111 (size: 7.3 KiB, free: 434.4 MiB)\n",
      "2023-04-29 00:01:24,331 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n",
      "2023-04-29 00:01:24,331 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-04-29 00:01:24,331 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "2023-04-29 00:01:24,332 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (localhost, executor 1, partition 0, PROCESS_LOCAL, 7391 bytes) \n",
      "2023-04-29 00:01:24,341 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:39805 (size: 7.3 KiB, free: 434.4 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+\n",
      "| id|lowerCaseString(name)|\n",
      "+---+---------------------+\n",
      "|  1|                alice|\n",
      "|  2|                braga|\n",
      "|  3|                steve|\n",
      "+---+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 00:01:24,407 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 75 ms on localhost (executor 1) (1/1)\n",
      "2023-04-29 00:01:24,407 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2023-04-29 00:01:24,408 INFO scheduler.DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.082 s\n",
      "2023-04-29 00:01:24,409 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-04-29 00:01:24,409 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\n",
      "2023-04-29 00:01:24,409 INFO scheduler.DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.085547 s\n",
      "2023-04-29 00:01:24,417 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-04-29 00:01:24,417 INFO scheduler.DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-04-29 00:01:24,418 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-04-29 00:01:24,418 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-04-29 00:01:24,418 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-04-29 00:01:24,418 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-04-29 00:01:24,421 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.1 KiB, free 434.3 MiB)\n",
      "2023-04-29 00:01:24,423 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 434.2 MiB)\n",
      "2023-04-29 00:01:24,424 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:37111 (size: 7.3 KiB, free: 434.3 MiB)\n",
      "2023-04-29 00:01:24,424 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
      "2023-04-29 00:01:24,424 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
      "2023-04-29 00:01:24,424 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "2023-04-29 00:01:24,427 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (localhost, executor 1, partition 1, PROCESS_LOCAL, 7428 bytes) \n",
      "2023-04-29 00:01:24,440 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:39805 (size: 7.3 KiB, free: 434.3 MiB)\n",
      "2023-04-29 00:01:24,496 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 70 ms on localhost (executor 1) (1/1)\n",
      "2023-04-29 00:01:24,496 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2023-04-29 00:01:24,497 INFO scheduler.DAGScheduler: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 0.078 s\n",
      "2023-04-29 00:01:24,497 INFO scheduler.DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-04-29 00:01:24,497 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\n",
      "2023-04-29 00:01:24,498 INFO scheduler.DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 0.080827 s\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT id, lowerCaseString(name)\n",
    "    FROM users\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f06df1-15b9-4e29-b15e-fb8c996f20f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e438da3-6b26-4d69-97eb-01aaf0ce4586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e26549ad-0453-4d16-bc8f-1fb95c02da67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Access via DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac7762e-6d07-4c5c-a0c4-6a5fe14e00b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "from pyspark.sql.column import Column, _to_java_column, _to_seq \n",
    "\n",
    "def greater_than_zero_udf(df_col: str):\n",
    "    greaterThanZeroUDF = spark.sparkContext._jvm.com.myudfs.MathUDFs.isGreaterThanZeroUDF()\n",
    "    return Column(greaterThanZeroUDF.apply(_to_seq(spark.sparkContext, [df_col], _to_java_column)))\n",
    "\n",
    "def multiplier_udf(df_col: str, multiply_by: int):\n",
    "    def multiplier(df_col: str):\n",
    "        multiplierUDF = spark.sparkContext._jvm.com.myudfs.MathUDFs.multiplyBy(multiply_by)\n",
    "        return Column(multiplierUDF.apply(_to_seq(spark.sparkContext, [df_col], _to_java_column)))\n",
    "\n",
    "    return multiplier(df_col)\n",
    "\n",
    "# work in progress\n",
    "def lower_case_udf(df_col: str):\n",
    "    lower_case_class = spark.sparkContext._jvm.com.myudfs.LowerCaseString\n",
    "    return Column(lower_case_class.apply(_to_seq(spark.sparkContext, [df_col], _to_java_column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80efa0e-772d-4e0f-8f4e-ec3fff8241a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 00:01:24,591 INFO codegen.CodeGenerator: Code generated in 12.876954 ms\n",
      "2023-04-29 00:01:24,600 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-04-29 00:01:24,601 INFO scheduler.DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-04-29 00:01:24,601 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-04-29 00:01:24,601 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-04-29 00:01:24,602 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-04-29 00:01:24,602 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-04-29 00:01:24,606 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 29.3 KiB, free 434.2 MiB)\n",
      "2023-04-29 00:01:24,608 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 13.3 KiB, free 434.2 MiB)\n",
      "2023-04-29 00:01:24,609 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37111 (size: 13.3 KiB, free: 434.3 MiB)\n",
      "2023-04-29 00:01:24,609 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n",
      "2023-04-29 00:01:24,610 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-04-29 00:01:24,610 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "2023-04-29 00:01:24,611 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (localhost, executor 1, partition 0, PROCESS_LOCAL, 7391 bytes) \n",
      "2023-04-29 00:01:24,622 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:39805 (size: 13.3 KiB, free: 434.3 MiB)\n",
      "2023-04-29 00:01:24,679 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 68 ms on localhost (executor 1) (1/1)\n",
      "2023-04-29 00:01:24,679 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "2023-04-29 00:01:24,680 INFO scheduler.DAGScheduler: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 0.077 s\n",
      "2023-04-29 00:01:24,680 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-04-29 00:01:24,680 INFO cluster.YarnScheduler: Killing all running tasks in stage 6: Stage finished\n",
      "2023-04-29 00:01:24,681 INFO scheduler.DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.080856 s\n",
      "2023-04-29 00:01:24,690 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "2023-04-29 00:01:24,691 INFO scheduler.DAGScheduler: Got job 7 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-04-29 00:01:24,691 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0)\n",
      "2023-04-29 00:01:24,691 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-04-29 00:01:24,691 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-04-29 00:01:24,692 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-04-29 00:01:24,695 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 29.3 KiB, free 434.2 MiB)\n",
      "2023-04-29 00:01:24,697 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 13.3 KiB, free 434.2 MiB)\n",
      "2023-04-29 00:01:24,697 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37111 (size: 13.3 KiB, free: 434.3 MiB)\n",
      "2023-04-29 00:01:24,698 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535\n",
      "2023-04-29 00:01:24,698 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
      "2023-04-29 00:01:24,698 INFO cluster.YarnScheduler: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "2023-04-29 00:01:24,699 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (localhost, executor 2, partition 1, PROCESS_LOCAL, 7428 bytes) \n",
      "2023-04-29 00:01:24,856 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:39405 (size: 13.3 KiB, free: 434.4 MiB)\n",
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------------+----------+\n",
      "| name| id|greater_than_zero|multiplied|\n",
      "+-----+---+-----------------+----------+\n",
      "|Alice|  1|             true|        10|\n",
      "|Braga|  2|             true|        20|\n",
      "|Steve|  3|             true|        30|\n",
      "+-----+---+-----------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 00:01:26,485 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1785 ms on localhost (executor 2) (1/1)\n",
      "2023-04-29 00:01:26,485 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "2023-04-29 00:01:26,486 INFO scheduler.DAGScheduler: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 1.793 s\n",
      "2023-04-29 00:01:26,486 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-04-29 00:01:26,486 INFO cluster.YarnScheduler: Killing all running tasks in stage 7: Stage finished\n",
      "2023-04-29 00:01:26,486 INFO scheduler.DAGScheduler: Job 7 finished: showString at NativeMethodAccessorImpl.java:0, took 1.795666 s\n",
      "2023-04-29 00:01:26,500 INFO codegen.CodeGenerator: Code generated in 9.150659 ms\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn(\"greater_than_zero\", greater_than_zero_udf(\"id\")) \\\n",
    "    .withColumn(\"multiplied\", multiplier_udf(\"id\", 10)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539455e1-547a-4785-b126-28298425cb09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
