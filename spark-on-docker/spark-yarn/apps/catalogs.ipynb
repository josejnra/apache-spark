{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aebfbd1-20e0-4591-93de-dd25c01f05cc",
   "metadata": {},
   "source": [
    "## In Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb5fd0c-dc64-4294-83b5-b5db53283afc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n'CreateTable `spark_catalog`.`default`.`example_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHive Catalog\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.warehouse.dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/spark-warehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Criar uma tabela tempor치ria em mem칩ria\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE TABLE example_table (id INT, name STRING)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n'CreateTable `spark_catalog`.`default`.`example_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar a SparkSession sem suporte ao Hive\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hive Catalog\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark\n",
    "\n",
    "# Criar uma tabela tempor치ria em mem칩ria\n",
    "spark.sql(\"CREATE TABLE example_table (id INT, name STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7050a-7aed-4868-bb9d-049f5a93aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listCatalogs()\n",
    "\n",
    "# Databases in metastore\n",
    "spark.sql(\"\"\"\n",
    "    SHOW databases\n",
    "\"\"\").toPandas()\n",
    "\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28fdcc-6a1d-4158-9d20-8d672688a6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bde18-d4e4-43e9-8f73-f0bd9894132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_list_data = [('Alice', 1), ('John', 2), ('Steve', 3)]\n",
    "spark.createDataFrame(tuple_list_data, ['name', 'id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198be61-2a71-44a2-b3ea-db2762f11317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114980f4-45e4-41bc-bd2f-5f964167f42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43c341ba-932e-4cc7-9cba-327084ca4bde",
   "metadata": {},
   "source": [
    "## Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c001a-a5ba-48c4-ad05-48db5a1d9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar a SparkSession com suporte ao Hive\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Hive Metastore\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE hive_table (\n",
    "    id INT,\n",
    "    name STRING\n",
    ") USING hive\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb789de-8b8c-4369-b1c2-05ff13c0f760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85ebaf-d593-4997-9e43-c4e574b3a0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c6ad2-c143-412b-9300-58e5c717c996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909a34b-630c-4658-aa9e-7605d2bc572f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c0209-5891-4ba4-8aba-2b17d26ddf20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346d0fa-0259-4ab2-ad20-2cd8790665a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca7e698-03c8-46e8-af98-862341216e5f",
   "metadata": {},
   "source": [
    "## Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b92df6-cc91-4a69-84fa-daf1b29f5b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-04 20:55:15,492 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted /delta-warehouse\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /delta-warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e3bcab-854d-4589-a7ac-102a5158bdfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-without-hadoop/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7a7e0430-dd4c-4565-9539-e0805d117ba1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 80ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7a7e0430-dd4c-4565-9539-e0805d117ba1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>delta lake</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff51395520>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "warehouse_path = \"/delta-warehouse\"  # path to warehouse in HDFS hdfs://localhost:9000/delta-warehouse\n",
    "enable_schema_merge = \"true\"\n",
    "type_compatibility_policy = \"LEGACY\"  # legacy enables more flexibility\n",
    "\n",
    "# Inicializar a SparkSession com suporte ao Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"delta lake\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_path) \\\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", enable_schema_merge) \\\n",
    "    .config(\"spark.sql.storeAssignmentPolicy\", type_compatibility_policy) \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1eb35aa-e050-4fbb-b676-9bb7b6d42664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catalog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spark_catalog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         catalog\n",
       "0  spark_catalog"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SHOW catalogs\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7118d7c-d034-40c6-b50f-ac72b758f5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_catalog'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentCatalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5e6126-cec7-4461-9eba-fcf35bc30b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS mydb\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2236b2a-92fb-485e-85ff-5ea08be4a89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS another_db\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bce2e7a-4cc9-4e73-adf6-a52585ae758f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>another_db</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mydb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    namespace\n",
       "0  another_db\n",
       "1     default\n",
       "2        mydb"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SHOW databases\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05a52767-783c-46c9-a608-f9dc383c6c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar uma tabela Delta Lake\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE mydb.delta_table (\n",
    "    id INT COMMENT 'Identificador 칰nico',\n",
    "    name STRING COMMENT 'Nome do indiv칤duo'\n",
    ") USING delta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "061aca00-f2f2-4150-ab5e-2928c19f61d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>lastModified</th>\n",
       "      <th>partitionColumns</th>\n",
       "      <th>clusteringColumns</th>\n",
       "      <th>numFiles</th>\n",
       "      <th>sizeInBytes</th>\n",
       "      <th>properties</th>\n",
       "      <th>minReaderVersion</th>\n",
       "      <th>minWriterVersion</th>\n",
       "      <th>tableFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delta</td>\n",
       "      <td>812b9bda-93d7-4344-ab48-7b85ceaa5ccf</td>\n",
       "      <td>spark_catalog.mydb.delta_table_2</td>\n",
       "      <td>None</td>\n",
       "      <td>hdfs://localhost:9000/delta-warehouse/mydb.db/...</td>\n",
       "      <td>2024-08-04 20:58:39.577</td>\n",
       "      <td>2024-08-04 20:58:40.850</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[appendOnly, invariants]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  format                                    id  \\\n",
       "0  delta  812b9bda-93d7-4344-ab48-7b85ceaa5ccf   \n",
       "\n",
       "                               name description  \\\n",
       "0  spark_catalog.mydb.delta_table_2        None   \n",
       "\n",
       "                                            location               createdAt  \\\n",
       "0  hdfs://localhost:9000/delta-warehouse/mydb.db/... 2024-08-04 20:58:39.577   \n",
       "\n",
       "             lastModified partitionColumns clusteringColumns  numFiles  \\\n",
       "0 2024-08-04 20:58:40.850               []                []         0   \n",
       "\n",
       "   sizeInBytes properties  minReaderVersion  minWriterVersion  \\\n",
       "0            0         {}                 1                 2   \n",
       "\n",
       "              tableFeatures  \n",
       "0  [appendOnly, invariants]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Criar outra tabela delta\n",
    "DeltaTable.createOrReplace(spark) \\\n",
    "    .tableName(\"mydb.delta_table_2\") \\\n",
    "    .addColumn(\"id\", dataType=\"INT\", nullable=False, comment= \"Identificador 칰nico\") \\\n",
    "    .addColumn(\"name\", dataType=StringType(), comment=\"Identificador 칰nico\") \\\n",
    "    .execute() \\\n",
    "    .detail() \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbf11db9-7434-4072-bd3b-0544fab93959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mydb</td>\n",
       "      <td>delta_table</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mydb</td>\n",
       "      <td>delta_table_2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  namespace      tableName  isTemporary\n",
       "0      mydb    delta_table        False\n",
       "1      mydb  delta_table_2        False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SHOW tables in mydb\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c50327cf-99b9-44ae-bc86-7542f1b742a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>int</td>\n",
       "      <td>Identificador 칰nico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name</td>\n",
       "      <td>string</td>\n",
       "      <td>Nome do indiv칤duo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># Detailed Table Information</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Name</td>\n",
       "      <td>spark_catalog.mydb.delta_table</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Type</td>\n",
       "      <td>MANAGED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Location</td>\n",
       "      <td>hdfs://localhost:9000/delta-warehouse/mydb.db/...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Provider</td>\n",
       "      <td>delta</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Table Properties</td>\n",
       "      <td>[delta.minReaderVersion=1,delta.minWriterVersi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       col_name  \\\n",
       "0                            id   \n",
       "1                          name   \n",
       "2                                 \n",
       "3  # Detailed Table Information   \n",
       "4                          Name   \n",
       "5                          Type   \n",
       "6                      Location   \n",
       "7                      Provider   \n",
       "8              Table Properties   \n",
       "\n",
       "                                           data_type              comment  \n",
       "0                                                int  Identificador 칰nico  \n",
       "1                                             string    Nome do indiv칤duo  \n",
       "2                                                                          \n",
       "3                                                                          \n",
       "4                     spark_catalog.mydb.delta_table                       \n",
       "5                                            MANAGED                       \n",
       "6  hdfs://localhost:9000/delta-warehouse/mydb.db/...                       \n",
       "7                                              delta                       \n",
       "8  [delta.minReaderVersion=1,delta.minWriterVersi...                       "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    describe table extended mydb.delta_table\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bfd5426-6780-40d8-afaf-9d8739f1bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO mydb.delta_table VALUES (1, \"John\"), (2, \"Maria\")\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd931de3-6766-461e-abbf-8219287a0d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True)\n",
    "])\n",
    "\n",
    "tuple_list_data = [(1, 'Alice'), (2, 'John'), (3, 'Steve')]\n",
    "df = spark.createDataFrame(tuple_list_data, schema=schema)\n",
    "\n",
    "df.write \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"mydb.delta_table\", format=\"delta\", mode=\"overwrite\")\n",
    "\n",
    "# df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"mydb.delta_table\")\n",
    "\n",
    "# df.write \\\n",
    "#   .format(\"delta\") \\\n",
    "#   .mode(\"overwrite\") \\\n",
    "#   .saveAsTable(\"mydb.delta_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ffc154-2d3a-465a-bd1c-1d042c38e63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"mydb.delta_table\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1298a-8cc1-4e1b-a548-44e15d6f7c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f8bc6-3379-48a0-aed8-8ce82611624c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb45d388-9bf5-47fe-b06d-e3c81af4f497",
   "metadata": {},
   "source": [
    "## Iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686cf81-01d5-483b-ac4b-ea8f7b7d67c9",
   "metadata": {},
   "source": [
    "A catalog is created and named by adding a property spark.sql.catalog.(catalog-name) with an implementation class for its value.\n",
    "\n",
    "Iceberg supplies two implementations:\n",
    "\n",
    "- **org.apache.iceberg.spark.SparkCatalog** supports a Hive Metastore or a Hadoop warehouse as a catalog\n",
    "- **org.apache.iceberg.spark.SparkSessionCatalog** adds support for Iceberg tables to Spark's built-in catalog, and delegates to the built-in catalog for non-Iceberg table\n",
    "\n",
    "Reference: https://iceberg.apache.org/docs/latest/spark-configuration/#catalog-configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90c8fbb-7030-4d60-a1bc-3fa85234cce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-04 13:46:17,773 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted /iceberg-warehouse\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /iceberg-warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c76bf4d1-5b94-489a-a44a-8e50e31cd60c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-without-hadoop/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c68c5838-ae14-4ee0-8a5a-54c0a6d402c5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.0 in central\n",
      ":: resolution report :: resolve 52ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c68c5838-ae14-4ee0-8a5a-54c0a6d402c5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>iceberg</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff88aee4f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# catalog_type = \"hive\"  # tables from a Hive metastore\n",
    "catalog_type = \"hadoop\"  # directory-based catalog in HDFS\n",
    "warehouse_path = \"/iceberg-warehouse\"  # path to warehouse in HDFS hdfs://localhost:9000/iceberg-warehouse\n",
    "\n",
    "# Inicializar a SparkSession com suporte ao Iceberg\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"iceberg\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_catalog.type\", catalog_type) \\\n",
    "    .config(\"spark.sql.catalog.iceberg_catalog.warehouse\", warehouse_path) \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"iceberg_catalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark\n",
    "\n",
    "# another approach\n",
    "# .config(\"spark.sql.catalog.iceberg_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "# .config(\"spark.sql.catalog.iceberg_catalog.type\", \"hive\")\n",
    "\n",
    "# remote catalog\n",
    "# .config(\"spark.sql.catalog.iceberg_catalog.uri\", \"thrift://localhost:9083\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f915e376-b15f-4fa7-b12d-a6a1296a667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.catalog.setCurrentCatalog(\"iceberg_catalog\")\n",
    "\n",
    "# OR by setting up\n",
    "\n",
    "# .config(\"spark.sql.defaultCatalog\", \"iceberg_catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff133bd-6025-42f8-b07b-5fa0f6145f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catalog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iceberg_catalog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spark_catalog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           catalog\n",
       "0  iceberg_catalog\n",
       "1    spark_catalog"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SHOW catalogs\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8b6ec2-aa4f-4c2e-b937-c807c2fd942d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iceberg_catalog'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentCatalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3423c022-dc93-4f35-9b96-1b9bc6de968f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg_catalog.mydb\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3fd0de-fd04-4c2a-a220-feb4e6073e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mydb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  namespace\n",
       "0      mydb"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SHOW databases\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7f1e6f-c222-4dd2-a01e-6763bf459575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar uma tabela Iceberg\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE mydb.iceberg_table (\n",
    "    id INT COMMENT 'Identificador 칰nico',\n",
    "    name STRING COMMENT 'Nome do indiv칤duo'\n",
    ") USING iceberg\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8baf8c6-c1af-4654-9fcd-a3e0405a22b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mydb</td>\n",
       "      <td>iceberg_table</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  namespace      tableName  isTemporary\n",
       "0      mydb  iceberg_table        False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SHOW tables in mydb\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87cff1dd-0d97-4795-b596-34308d446757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>int</td>\n",
       "      <td>Identificador 칰nico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name</td>\n",
       "      <td>string</td>\n",
       "      <td>Nome do indiv칤duo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># Metadata Columns</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_spec_id</td>\n",
       "      <td>int</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>_partition</td>\n",
       "      <td>struct&lt;&gt;</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>_file</td>\n",
       "      <td>string</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>_pos</td>\n",
       "      <td>bigint</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>_deleted</td>\n",
       "      <td>boolean</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td># Detailed Table Information</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Name</td>\n",
       "      <td>iceberg_catalog.mydb.iceberg_table</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Type</td>\n",
       "      <td>MANAGED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Location</td>\n",
       "      <td>/iceberg-warehouse/mydb/iceberg_table</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Provider</td>\n",
       "      <td>iceberg</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Owner</td>\n",
       "      <td>root</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Table Properties</td>\n",
       "      <td>[current-snapshot-id=none,format=iceberg/parqu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        col_name  \\\n",
       "0                             id   \n",
       "1                           name   \n",
       "2                                  \n",
       "3             # Metadata Columns   \n",
       "4                       _spec_id   \n",
       "5                     _partition   \n",
       "6                          _file   \n",
       "7                           _pos   \n",
       "8                       _deleted   \n",
       "9                                  \n",
       "10  # Detailed Table Information   \n",
       "11                          Name   \n",
       "12                          Type   \n",
       "13                      Location   \n",
       "14                      Provider   \n",
       "15                         Owner   \n",
       "16              Table Properties   \n",
       "\n",
       "                                            data_type              comment  \n",
       "0                                                 int  Identificador 칰nico  \n",
       "1                                              string    Nome do indiv칤duo  \n",
       "2                                                                           \n",
       "3                                                                           \n",
       "4                                                 int                       \n",
       "5                                            struct<>                       \n",
       "6                                              string                       \n",
       "7                                              bigint                       \n",
       "8                                             boolean                       \n",
       "9                                                                           \n",
       "10                                                                          \n",
       "11                 iceberg_catalog.mydb.iceberg_table                       \n",
       "12                                            MANAGED                       \n",
       "13              /iceberg-warehouse/mydb/iceberg_table                       \n",
       "14                                            iceberg                       \n",
       "15                                               root                       \n",
       "16  [current-snapshot-id=none,format=iceberg/parqu...                       "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    describe table extended mydb.iceberg_table\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e796019-6ea0-4893-8f29-dd30390e2a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO mydb.iceberg_table VALUES (1, \"John\"), (2, \"Maria\")\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e087ea1-83bf-45ae-9673-61afd06af89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Maria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   name\n",
       "0   1   John\n",
       "1   2  Maria"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table(\"mydb.iceberg_table\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a02b3-6ed8-4720-a3ad-63f373ae2d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76757ded-45a2-4520-8051-4cf7df268519",
   "metadata": {},
   "source": [
    "## Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da8740-400e-4659-95de-bc1adf375e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar a SparkSession com suporte ao Glue Catalog\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Glue Catalog Example\") \\\n",
    "    .config(\"spark.sql.catalog.glue_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.glue_catalog.warehouse\", \"s3://my-glue-warehouse/\") \\\n",
    "    .config(\"spark.sql.catalog.glue_catalog.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.glue_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.catalog.glue_catalog.lock-impl\", \"org.apache.iceberg.aws.glue.DynamoLockManager\") \\\n",
    "    .config(\"spark.sql.catalog.glue_catalog.lock.table\", \"myGlueLockTable\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5f44d-33a6-4745-aa2f-9dc18bc55313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebbbe6b-a1cd-4137-84b2-3526d6807f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91143816-7d0a-4ba3-88c7-14811676c7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
